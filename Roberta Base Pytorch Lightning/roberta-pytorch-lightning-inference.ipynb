{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b38a02",
   "metadata": {
    "papermill": {
     "duration": 0.01281,
     "end_time": "2021-07-28T00:16:14.299934",
     "exception": false,
     "start_time": "2021-07-28T00:16:14.287124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Versions\n",
    "\n",
    "* Version 1: CV:- 0.4875 LB:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323d994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:42:55.355804Z",
     "iopub.status.busy": "2021-07-20T18:42:55.355046Z",
     "iopub.status.idle": "2021-07-20T18:42:55.360324Z",
     "shell.execute_reply": "2021-07-20T18:42:55.35926Z",
     "shell.execute_reply.started": "2021-07-20T18:42:55.355757Z"
    },
    "papermill": {
     "duration": 0.011119,
     "end_time": "2021-07-28T00:16:14.322675",
     "exception": false,
     "start_time": "2021-07-28T00:16:14.311556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd376a6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-28T00:16:14.358851Z",
     "iopub.status.busy": "2021-07-28T00:16:14.358195Z",
     "iopub.status.idle": "2021-07-28T00:16:22.594765Z",
     "shell.execute_reply": "2021-07-28T00:16:22.593757Z",
     "shell.execute_reply.started": "2021-07-28T00:15:29.828271Z"
    },
    "papermill": {
     "duration": 8.260763,
     "end_time": "2021-07-28T00:16:22.594924",
     "exception": false,
     "start_time": "2021-07-28T00:16:14.334161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#General\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.model_selection import train_test_split , KFold\n",
    "\n",
    "#Pytorch\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Pytorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything , Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#Hugging Face\n",
    "from transformers import AutoModel , AutoConfig , AutoTokenizer ,  AdamW, get_linear_schedule_with_warmup,get_constant_schedule_with_warmup,get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e54050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T00:16:22.622469Z",
     "iopub.status.busy": "2021-07-28T00:16:22.621717Z",
     "iopub.status.idle": "2021-07-28T00:16:22.793294Z",
     "shell.execute_reply": "2021-07-28T00:16:22.792144Z",
     "shell.execute_reply.started": "2021-07-28T00:15:29.842227Z"
    },
    "papermill": {
     "duration": 0.186944,
     "end_time": "2021-07-28T00:16:22.793432",
     "exception": false,
     "start_time": "2021-07-28T00:16:22.606488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = glob(\"../input/commonlitreadabilityprize/*.csv\")\n",
    "paths = sorted(paths)\n",
    "\n",
    "df_ss = pd.read_csv(paths[0])\n",
    "df_test = pd.read_csv(paths[1])\n",
    "df_train = pd.read_csv(paths[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ccc48c",
   "metadata": {
    "papermill": {
     "duration": 0.011251,
     "end_time": "2021-07-28T00:16:22.816317",
     "exception": false,
     "start_time": "2021-07-28T00:16:22.805066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34394520",
   "metadata": {
    "papermill": {
     "duration": 0.011142,
     "end_time": "2021-07-28T00:16:22.838680",
     "exception": false,
     "start_time": "2021-07-28T00:16:22.827538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0d0615a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T00:16:22.908967Z",
     "iopub.status.busy": "2021-07-28T00:16:22.908366Z",
     "iopub.status.idle": "2021-07-28T00:16:22.920640Z",
     "shell.execute_reply": "2021-07-28T00:16:22.921090Z",
     "shell.execute_reply.started": "2021-07-28T00:15:29.935549Z"
    },
    "papermill": {
     "duration": 0.070847,
     "end_time": "2021-07-28T00:16:22.921222",
     "exception": false,
     "start_time": "2021-07-28T00:16:22.850375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class config:\n",
    "    seed = 123\n",
    "    batch_size = 16\n",
    "    epochs = 20\n",
    "    transformer_name = \"roberta-base\"\n",
    "    transformer_path = \"../input/huggingface-roberta/roberta-base\"\n",
    "    max_len = 250\n",
    "    learning_rate = 2e-5\n",
    "    save_dir = \"./result\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3503d4c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T00:16:22.948538Z",
     "iopub.status.busy": "2021-07-28T00:16:22.947847Z",
     "iopub.status.idle": "2021-07-28T00:16:22.950546Z",
     "shell.execute_reply": "2021-07-28T00:16:22.950060Z",
     "shell.execute_reply.started": "2021-07-28T00:15:29.952807Z"
    },
    "papermill": {
     "duration": 0.017663,
     "end_time": "2021-07-28T00:16:22.950660",
     "exception": false,
     "start_time": "2021-07-28T00:16:22.932997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(config.save_dir):\n",
    "    os.makedirs(config.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3333ad",
   "metadata": {
    "papermill": {
     "duration": 0.011413,
     "end_time": "2021-07-28T00:16:22.973459",
     "exception": false,
     "start_time": "2021-07-28T00:16:22.962046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1a8951",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T00:16:23.002768Z",
     "iopub.status.busy": "2021-07-28T00:16:23.002089Z",
     "iopub.status.idle": "2021-07-28T00:16:23.004906Z",
     "shell.execute_reply": "2021-07-28T00:16:23.004484Z",
     "shell.execute_reply.started": "2021-07-28T00:15:30.029205Z"
    },
    "papermill": {
     "duration": 0.019995,
     "end_time": "2021-07-28T00:16:23.005009",
     "exception": false,
     "start_time": "2021-07-28T00:16:22.985014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLRDataset:\n",
    "    def __init__(self, name , dataset):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "        self.max_len = config.max_len\n",
    "        \n",
    "        self.excerpt = dataset['excerpt'].to_numpy()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)\n",
    "    \n",
    "    def __getitem__(self , idx):\n",
    "        text = self.excerpt[idx]\n",
    "        tokenized_text = self.tokenizer(text, truncation = True , padding = \"max_length\" , max_length= self.max_len )\n",
    "        \n",
    "        return {'input_ids': torch.tensor(tokenized_text['input_ids'], dtype = torch.long),\n",
    "                'attention_mask' : torch.tensor(tokenized_text['attention_mask'] , dtype = torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f0331c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T00:16:23.032700Z",
     "iopub.status.busy": "2021-07-28T00:16:23.032171Z",
     "iopub.status.idle": "2021-07-28T00:16:23.252105Z",
     "shell.execute_reply": "2021-07-28T00:16:23.251386Z",
     "shell.execute_reply.started": "2021-07-28T00:15:30.046103Z"
    },
    "papermill": {
     "duration": 0.235632,
     "end_time": "2021-07-28T00:16:23.252245",
     "exception": false,
     "start_time": "2021-07-28T00:16:23.016613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = df_train.loc[:]\n",
    "test_data = df_test.loc[:]\n",
    "\n",
    "train = CLRDataset(config.transformer_path,train_data)\n",
    "test = CLRDataset(config.transformer_path,test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train , batch_size = config.batch_size , shuffle = True , num_workers=4,pin_memory=False)\n",
    "test_dataloader =  DataLoader(test , batch_size = config.batch_size , shuffle = False , num_workers=4,pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbfc3f1",
   "metadata": {
    "papermill": {
     "duration": 0.01161,
     "end_time": "2021-07-28T00:16:23.275890",
     "exception": false,
     "start_time": "2021-07-28T00:16:23.264280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c9d0a4d",
   "metadata": {
    "papermill": {
     "duration": 0.011364,
     "end_time": "2021-07-28T00:16:23.298854",
     "exception": false,
     "start_time": "2021-07-28T00:16:23.287490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520b313d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T00:16:23.326504Z",
     "iopub.status.busy": "2021-07-28T00:16:23.325756Z",
     "iopub.status.idle": "2021-07-28T00:16:23.328481Z",
     "shell.execute_reply": "2021-07-28T00:16:23.328075Z",
     "shell.execute_reply.started": "2021-07-28T00:15:30.226595Z"
    },
    "papermill": {
     "duration": 0.018064,
     "end_time": "2021-07-28T00:16:23.328605",
     "exception": false,
     "start_time": "2021-07-28T00:16:23.310541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_function(output,target):\n",
    "    return torch.sqrt(nn.MSELoss()(output,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81dea176",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T00:16:23.369969Z",
     "iopub.status.busy": "2021-07-28T00:16:23.359258Z",
     "iopub.status.idle": "2021-07-28T00:16:23.379005Z",
     "shell.execute_reply": "2021-07-28T00:16:23.379435Z",
     "shell.execute_reply.started": "2021-07-28T00:15:30.417858Z"
    },
    "papermill": {
     "duration": 0.039582,
     "end_time": "2021-07-28T00:16:23.379583",
     "exception": false,
     "start_time": "2021-07-28T00:16:23.340001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(768,512)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(512,1)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "    \n",
    "    def forward(self,input_tensors):\n",
    "        x = self.linear(input_tensors)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,name):\n",
    "        super().__init__()\n",
    "        self.transformer_config = AutoConfig.from_pretrained(name)\n",
    "        self.transformer_config.update({\"output_hidden_states\":True, \"hidden_dropout_prob\": 0.0,\"layer_norm_eps\": 1e-7}) \n",
    "        self.transformer_model = AutoModel.from_pretrained(name , config = self.transformer_config)\n",
    "        \n",
    "    def forward(self,input_ids, attention_mask):\n",
    "        transformer_output = self.transformer_model(input_ids = input_ids , attention_mask = attention_mask)\n",
    "        transformer_output = transformer_output.hidden_states[-1]\n",
    "        \n",
    "        return transformer_output\n",
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(768,1)\n",
    "    \n",
    "    def forward(self,input_tensors):\n",
    "        x = self.linear(input_tensors)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class CLRModel(pl.LightningModule):\n",
    "    def __init__(self,name , train , validation):\n",
    "        super().__init__()\n",
    "        self._train_dataloader = train\n",
    "        self._val_dataloader = validation\n",
    "        self._test_dataloader = validation\n",
    "        self.name = name\n",
    "        self.transformer_model = TransformerModel(self.name)\n",
    "        self.regression_head = RegressionHead()\n",
    "        self.attention_head = AttentionHead()\n",
    "        self.predictions = []\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self,input_ids , attention_mask):\n",
    "        last_layer_hidden_states = self.transformer_model(input_ids , attention_mask)\n",
    "        weights = self.attention_head(last_layer_hidden_states) \n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)  \n",
    "        output = self.regression_head(context_vector)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target = batch['target']\n",
    "        \n",
    "\n",
    "        output = self.forward(input_ids , attention_mask)\n",
    "        loss = loss_function(output,target) \n",
    "        self.log('train_loss', loss , prog_bar=True)\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def train_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        print(f'epoch {trainer.current_epoch} training loss {avg_loss}')\n",
    "        return {'train_loss': avg_loss}    \n",
    "    \n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target = batch['target']\n",
    "        \n",
    "\n",
    "        output = self.forward(input_ids , attention_mask)\n",
    "        loss = loss_function(output,target) \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return {'val_loss': loss}\n",
    "    \n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        print(f'epoch {trainer.current_epoch} validation loss {avg_loss}')\n",
    "        return {'val_loss': avg_loss}\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        output = self.forward(input_ids , attention_mask)\n",
    "        self.predictions.append(output)\n",
    "        return self.predictions\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self._val_dataloader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self._test_dataloader\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        named_parameters = list(model.named_parameters())    \n",
    "    \n",
    "        roberta_parameters = named_parameters[:197]    \n",
    "        attention_parameters = named_parameters[199:203]\n",
    "        regressor_parameters = named_parameters[203:]\n",
    "\n",
    "        attention_group = [params for (name, params) in attention_parameters]\n",
    "        regressor_group = [params for (name, params) in regressor_parameters]\n",
    "        parameters = []\n",
    "        parameters.append({\"params\": attention_group})\n",
    "        parameters.append({\"params\": regressor_group})\n",
    "\n",
    "        for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "            weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "\n",
    "            lr = 2e-5\n",
    "\n",
    "            if layer_num >= 69:        \n",
    "                lr = 5e-5\n",
    "\n",
    "            if layer_num >= 133:\n",
    "                lr = 1e-4\n",
    "\n",
    "            parameters.append({\"params\": params,\n",
    "                               \"weight_decay\": weight_decay,\n",
    "                               \"lr\": lr})\n",
    "        \n",
    "        optimizer = AdamW(parameters)            \n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_training_steps= config.epochs * len(train_dataloader),\n",
    "            num_warmup_steps=50)\n",
    "            \n",
    "\n",
    "        \n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19626f0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T00:16:23.408294Z",
     "iopub.status.busy": "2021-07-28T00:16:23.407618Z",
     "iopub.status.idle": "2021-07-28T00:16:23.409992Z",
     "shell.execute_reply": "2021-07-28T00:16:23.410358Z",
     "shell.execute_reply.started": "2021-07-28T00:15:30.655622Z"
    },
    "papermill": {
     "duration": 0.01932,
     "end_time": "2021-07-28T00:16:23.410480",
     "exception": false,
     "start_time": "2021-07-28T00:16:23.391160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(data_loader, model):\n",
    "        \n",
    "    model.to(config.device)\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    predictions = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        inputs = {key:val.reshape(val.shape[0], -1).to(config.device) for key,val in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        predictions.extend(outputs.detach().cpu().numpy().ravel())\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c27f735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T00:16:23.438597Z",
     "iopub.status.busy": "2021-07-28T00:16:23.438081Z",
     "iopub.status.idle": "2021-07-28T00:17:14.233605Z",
     "shell.execute_reply": "2021-07-28T00:17:14.234161Z",
     "shell.execute_reply.started": "2021-07-28T00:15:30.672401Z"
    },
    "papermill": {
     "duration": 50.812214,
     "end_time": "2021-07-28T00:17:14.234366",
     "exception": false,
     "start_time": "2021-07-28T00:16:23.422152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** fold 1: ../input/robertaweightspytorch/result/fold_1.ckpt ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** fold 2: ../input/robertaweightspytorch/result/fold_2.ckpt ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** fold 3: ../input/robertaweightspytorch/result/fold_3.ckpt ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** fold 4: ../input/robertaweightspytorch/result/fold_4.ckpt ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** fold 5: ../input/robertaweightspytorch/result/fold_5.ckpt ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n"
     ]
    }
   ],
   "source": [
    "model_weight = glob(\"../input/robertaweightspytorch/result/*.ckpt\")\n",
    "model_weight = sorted(model_weight)\n",
    "\n",
    "fold_predictions = []\n",
    "fold =1\n",
    "for path in model_weight:\n",
    "    model = CLRModel.load_from_checkpoint(path)\n",
    "    print(f'*** fold {fold}: {path} ***')\n",
    "    y_pred = predict(test_dataloader, model)\n",
    "    fold_predictions.append(y_pred)\n",
    "    \n",
    "    fold +=1\n",
    "    # Free memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "predictions = np.mean(fold_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "672493ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T00:17:14.283343Z",
     "iopub.status.busy": "2021-07-28T00:17:14.282492Z",
     "iopub.status.idle": "2021-07-28T00:17:14.302164Z",
     "shell.execute_reply": "2021-07-28T00:17:14.301740Z",
     "shell.execute_reply.started": "2021-07-28T00:15:44.779513Z"
    },
    "papermill": {
     "duration": 0.047015,
     "end_time": "2021-07-28T00:17:14.302295",
     "exception": false,
     "start_time": "2021-07-28T00:17:14.255280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.314052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.507427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.495707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.372509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.795460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.314052\n",
       "1  f0953f0a5 -0.507427\n",
       "2  0df072751 -0.495707\n",
       "3  04caf4e0c -2.372509\n",
       "4  0e63f8bea -1.795460"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ss['target'] = predictions\n",
    "df_ss.to_csv('submission.csv', index=False)\n",
    "df_ss.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 69.697298,
   "end_time": "2021-07-28T00:17:16.867351",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-28T00:16:07.170053",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
