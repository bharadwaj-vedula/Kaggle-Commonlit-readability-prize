{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Versions\n\n* Version 1: CV:-0.0.4875 LB:-","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:42:55.355046Z","iopub.execute_input":"2021-07-20T18:42:55.355804Z","iopub.status.idle":"2021-07-20T18:42:55.360324Z","shell.execute_reply.started":"2021-07-20T18:42:55.355757Z","shell.execute_reply":"2021-07-20T18:42:55.35926Z"}}},{"cell_type":"code","source":"# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:58:52.851997Z","iopub.execute_input":"2021-07-27T04:58:52.852387Z","iopub.status.idle":"2021-07-27T04:58:52.856340Z","shell.execute_reply.started":"2021-07-27T04:58:52.852307Z","shell.execute_reply":"2021-07-27T04:58:52.855477Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:19:01.112602Z","iopub.execute_input":"2021-07-27T05:19:01.112978Z","iopub.status.idle":"2021-07-27T05:19:01.124814Z","shell.execute_reply.started":"2021-07-27T05:19:01.112896Z","shell.execute_reply":"2021-07-27T05:19:01.124084Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#General\nimport numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\nfrom glob import glob\nimport os\n\n#Sklearn\nfrom sklearn.model_selection import train_test_split , KFold\n\n#Pytorch\nimport torch \nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n#Pytorch Lightning\nimport pytorch_lightning as pl\nfrom pytorch_lightning import seed_everything , Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\n#Hugging Face\nfrom transformers import AutoModel , AutoConfig , AutoTokenizer ,  AdamW, get_linear_schedule_with_warmup,get_constant_schedule_with_warmup,get_cosine_schedule_with_warmup","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T05:19:01.391639Z","iopub.execute_input":"2021-07-27T05:19:01.391913Z","iopub.status.idle":"2021-07-27T05:19:09.508686Z","shell.execute_reply.started":"2021-07-27T05:19:01.391879Z","shell.execute_reply":"2021-07-27T05:19:09.507729Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"paths = glob(\"../input/commonlitreadabilityprize/*.csv\")\npaths = sorted(paths)\n\ndf_ss = pd.read_csv(paths[0])\ndf_test = pd.read_csv(paths[1])\ndf_train = pd.read_csv(paths[2])","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:19:09.512294Z","iopub.execute_input":"2021-07-27T05:19:09.512557Z","iopub.status.idle":"2021-07-27T05:19:09.642829Z","shell.execute_reply.started":"2021-07-27T05:19:09.512531Z","shell.execute_reply":"2021-07-27T05:19:09.641988Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"code","source":"class config:\n    seed = 123\n    batch_size = 16\n    epochs = 20\n    transformer_name = \"roberta-base\"\n    transformer_path = \"../input/huggingface-roberta/roberta-base\"\n    max_len = 250\n    learning_rate = 2e-5\n    save_dir = \"./result\"\n\nseed_everything(config.seed)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:19:09.646221Z","iopub.execute_input":"2021-07-27T05:19:09.646497Z","iopub.status.idle":"2021-07-27T05:19:09.659470Z","shell.execute_reply.started":"2021-07-27T05:19:09.646471Z","shell.execute_reply":"2021-07-27T05:19:09.658426Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"123"},"metadata":{}}]},{"cell_type":"code","source":"if not os.path.exists(config.save_dir):\n    os.makedirs(config.save_dir)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:19:09.661346Z","iopub.execute_input":"2021-07-27T05:19:09.661727Z","iopub.status.idle":"2021-07-27T05:19:09.666585Z","shell.execute_reply.started":"2021-07-27T05:19:09.661690Z","shell.execute_reply":"2021-07-27T05:19:09.665366Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CLRDataset:\n    def __init__(self, name , dataset):\n        self.tokenizer = AutoTokenizer.from_pretrained(name)\n        self.max_len = config.max_len\n        \n        self.excerpt = dataset['excerpt'].to_numpy()\n        self.target = dataset['target'].to_numpy()\n        \n\n    def __len__(self):\n        return len(self.excerpt)\n    \n    def __getitem__(self , idx):\n        text = self.excerpt[idx]\n        target = self.target[idx]\n        tokenized_text = self.tokenizer(text, truncation = True , padding = \"max_length\" , max_length= self.max_len )\n        \n        return {'input_ids': torch.tensor(tokenized_text['input_ids'], dtype = torch.long),\n                'attention_mask' : torch.tensor(tokenized_text['attention_mask'] , dtype = torch.long),\n               'target': torch.tensor([target],dtype = torch.float)}","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:19:09.668246Z","iopub.execute_input":"2021-07-27T05:19:09.668675Z","iopub.status.idle":"2021-07-27T05:19:09.678887Z","shell.execute_reply.started":"2021-07-27T05:19:09.668635Z","shell.execute_reply":"2021-07-27T05:19:09.677755Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def loss_function(output,target):\n    return torch.sqrt(nn.MSELoss()(output,target))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:19:09.680466Z","iopub.execute_input":"2021-07-27T05:19:09.681012Z","iopub.status.idle":"2021-07-27T05:19:09.690818Z","shell.execute_reply.started":"2021-07-27T05:19:09.680947Z","shell.execute_reply":"2021-07-27T05:19:09.689847Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(768,512)\n        self.tanh = nn.Tanh()\n        self.linear2 = nn.Linear(512,1)\n        self.softmax = nn.Softmax(dim = 1)\n    \n    def forward(self,input_tensors):\n        x = self.linear(input_tensors)\n        x = self.tanh(x)\n        x = self.linear2(x)\n        x = self.softmax(x)\n        \n        return x\n\nclass TransformerModel(nn.Module):\n    def __init__(self,name):\n        super().__init__()\n        self.transformer_config = AutoConfig.from_pretrained(name)\n        self.transformer_config.update({\"output_hidden_states\":True, \"hidden_dropout_prob\": 0.0,\"layer_norm_eps\": 1e-7}) \n        self.transformer_model = AutoModel.from_pretrained(name , config = self.transformer_config)\n        \n    def forward(self,input_ids, attention_mask):\n        transformer_output = self.transformer_model(input_ids = input_ids , attention_mask = attention_mask)\n        transformer_output = transformer_output.hidden_states[-1]\n        \n        return transformer_output\n\nclass RegressionHead(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(768,1)\n    \n    def forward(self,input_tensors):\n        x = self.linear(input_tensors)\n        \n        return x\n\nclass CLRModel(pl.LightningModule):\n    def __init__(self,name , train , validation):\n        super().__init__()\n        self._train_dataloader = train\n        self._val_dataloader = validation\n        self._test_dataloader = validation\n        self.name = name\n        self.transformer_model = TransformerModel(self.name)\n        self.regression_head = RegressionHead()\n        self.attention_head = AttentionHead()\n        self.save_hyperparameters()\n    \n    def forward(self,input_ids , attention_mask):\n        last_layer_hidden_states = self.transformer_model(input_ids , attention_mask)\n        weights = self.attention_head(last_layer_hidden_states) \n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)  \n        output = self.regression_head(context_vector)\n        \n        return output\n    \n    def training_step(self,batch,batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        target = batch['target']\n        \n\n        output = self.forward(input_ids , attention_mask)\n        loss = loss_function(output,target) \n        self.log('train_loss', loss , prog_bar=True)\n\n        return {'loss': loss}\n\n    def train_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        print(f'epoch {trainer.current_epoch} training loss {avg_loss}')\n        return {'train_loss': avg_loss}    \n    \n    \n    def validation_step(self,batch,batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        target = batch['target']\n        \n\n        output = self.forward(input_ids , attention_mask)\n        loss = loss_function(output,target) \n        self.log('val_loss', loss, prog_bar=True)\n        \n        return {'val_loss': loss}\n    \n\n    def validation_epoch_end(self, outputs):\n\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        print(f'epoch {trainer.current_epoch} validation loss {avg_loss}')\n        return {'val_loss': avg_loss}\n    \n    \n    def test_step(self, batch,batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        target = batch['target']\n\n        output = self.forward(input_ids , attention_mask)\n        loss = loss_function(output,target) \n        self.log('test_loss', loss)\n        return {'test_loss': loss}\n    \n    def test_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n        print(f'epoch {trainer.current_epoch} test loss {avg_loss}')\n        return {'test_loss': avg_loss}\n    \n    def train_dataloader(self):\n        return self._train_dataloader\n    \n    def val_dataloader(self):\n        return self._val_dataloader\n    \n    def test_dataloader(self):\n        return self._val_dataloader\n    \n    def configure_optimizers(self):\n        \"\"\"optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n        lr_scheduler = get_constant_schedule_with_warmup(optimizer, 100)\"\"\"\n        named_parameters = list(model.named_parameters())    \n    \n        roberta_parameters = named_parameters[:197]    \n        attention_parameters = named_parameters[199:203]\n        regressor_parameters = named_parameters[203:]\n\n        attention_group = [params for (name, params) in attention_parameters]\n        regressor_group = [params for (name, params) in regressor_parameters]\n        parameters = []\n        parameters.append({\"params\": attention_group})\n        parameters.append({\"params\": regressor_group})\n\n        for layer_num, (name, params) in enumerate(roberta_parameters):\n            weight_decay = 0.0 if \"bias\" in name else 0.01\n\n            lr = 2e-5\n\n            if layer_num >= 69:        \n                lr = 5e-5\n\n            if layer_num >= 133:\n                lr = 1e-4\n\n            parameters.append({\"params\": params,\n                               \"weight_decay\": weight_decay,\n                               \"lr\": lr})\n        \n        optimizer = AdamW(parameters)            \n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_training_steps= config.epochs * len(train_dataloader),\n            num_warmup_steps=50)\n            \n\n        \n        return [optimizer], [scheduler]\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:19:09.692345Z","iopub.execute_input":"2021-07-27T05:19:09.692897Z","iopub.status.idle":"2021-07-27T05:19:09.723722Z","shell.execute_reply.started":"2021-07-27T05:19:09.692848Z","shell.execute_reply":"2021-07-27T05:19:09.722817Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.callbacks import ProgressBarBase ,ProgressBar\n\n\nclass EpochProgressBar(ProgressBarBase):\n\n    def __init__(self):\n        super().__init__()\n        self.bar = None\n\n    def on_train_start(self, trainer, pl_module):\n        self.bar = tqdm(\n            desc='Epoch',\n            leave=False,\n            dynamic_ncols=True,\n            total=trainer.max_epochs,\n        )\n\n    def on_train_epoch_end(self, *args, **kwargs):\n        self.bar.update(1)\n\nclass LitProgressBar(ProgressBar):\n    def __init__(self):\n        super().__init__()\n        \n    def on_train_epoch_start(self, trainer, pl_module):\n        if trainer.current_epoch:\n            print()\n        super().on_train_epoch_start(trainer, pl_module)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:19:09.725877Z","iopub.execute_input":"2021-07-27T05:19:09.726273Z","iopub.status.idle":"2021-07-27T05:19:09.737508Z","shell.execute_reply.started":"2021-07-27T05:19:09.726236Z","shell.execute_reply":"2021-07-27T05:19:09.736563Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"scores=[]\niterations = 1\nkfold = KFold(n_splits=5, shuffle= True , random_state = config.seed)\nfor train_idx, test_idx in kfold.split(df_train):\n    print(\"************** iteration\",iterations,\"**************\")\n    \n    train_data = df_train.loc[train_idx]\n    validation_data = df_train.loc[test_idx]\n    \n    train = CLRDataset(config.transformer_path,train_data)\n    valid = CLRDataset(config.transformer_path,validation_data)\n    \n    train_dataloader = DataLoader(train , batch_size = config.batch_size , shuffle = True , num_workers=4,pin_memory=False)\n    validation_dataloader = DataLoader(valid , batch_size = config.batch_size , shuffle = False , num_workers=4,pin_memory=False)\n    checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n                                          dirpath= config.save_dir,\n                                      save_top_k=1,\n                                      save_last= False,\n                                      save_weights_only=True,\n                                      filename= f\"./fold_{iterations}\",\n                                      verbose= True,\n                                      mode='min')\n    \n    model = CLRModel(config.transformer_path, train_dataloader , validation_dataloader)\n    trainer = Trainer(max_epochs= config.epochs,gpus =1 , progress_bar_refresh_rate=10,callbacks=[checkpoint_callback])\n    trainer.fit(model , train_dataloader , validation_dataloader)\n    \n    #predictions\n    print('predicting')\n    model_test = CLRModel.load_from_checkpoint(f'result/fold_{iterations}.ckpt')\n    a = trainer.test(model_test)\n    \n    iterations +=1 ","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:19:42.454865Z","iopub.execute_input":"2021-07-27T05:19:42.455224Z","iopub.status.idle":"2021-07-27T07:09:36.130669Z","shell.execute_reply.started":"2021-07-27T05:19:42.455193Z","shell.execute_reply":"2021-07-27T07:09:36.129774Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"************** iteration 1 **************\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation sanity check: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94b2fac15eed43e485a9d4169ff2e413"}},"metadata":{}},{"name":"stdout","text":"epoch 0 validation loss 1.5043253898620605\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fbacfc5a05e4b3bb3b873d59e495460"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 0 validation loss 1.5343905687332153\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 1 validation loss 0.6941792964935303\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 2 validation loss 0.5855439305305481\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 3 validation loss 0.5505715608596802\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 4 validation loss 0.5471032857894897\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 5 validation loss 0.5356782078742981\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 6 validation loss 0.5748260617256165\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 7 validation loss 0.543407142162323\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 8 validation loss 0.5190924406051636\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 9 validation loss 0.5280966758728027\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 10 validation loss 0.5238869190216064\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 11 validation loss 0.5370643734931946\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 12 validation loss 0.5186502933502197\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 13 validation loss 0.5450088381767273\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 14 validation loss 0.5447915196418762\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 15 validation loss 0.5608595013618469\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 16 validation loss 0.5324256420135498\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 17 validation loss 0.5250145792961121\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 18 validation loss 0.5055640339851379\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 19 validation loss 0.5065579414367676\npredicting\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f747d1e60c749c495b8e8ce286dffe0"}},"metadata":{}},{"name":"stdout","text":"epoch 19 test loss 0.5055640339851379\n--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test_loss': 0.5041112899780273}\n--------------------------------------------------------------------------------\n************** iteration 2 **************\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation sanity check: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecc68d34bb14416a84041aa8aa592821"}},"metadata":{}},{"name":"stdout","text":"epoch 0 validation loss 0.7103824615478516\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7945cc6827cb4a95885eee695db0dfee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 0 validation loss 1.1000523567199707\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 1 validation loss 0.6558171510696411\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 2 validation loss 0.5495833158493042\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 3 validation loss 0.5141873359680176\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 4 validation loss 0.5229269862174988\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 5 validation loss 0.48832327127456665\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 6 validation loss 0.5002201795578003\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 7 validation loss 0.5138178467750549\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 8 validation loss 0.5132287740707397\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 9 validation loss 0.5438933968544006\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 10 validation loss 0.5090841054916382\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 11 validation loss 0.543070375919342\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 12 validation loss 0.4930374324321747\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 13 validation loss 0.5022643208503723\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 14 validation loss 0.49371662735939026\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 15 validation loss 0.5100337266921997\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 16 validation loss 0.48971787095069885\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 17 validation loss 0.49080732464790344\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 18 validation loss 0.4796077013015747\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 19 validation loss 0.4782487452030182\npredicting\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea91bf2cc6244ca6ac64f30f3681e811"}},"metadata":{}},{"name":"stdout","text":"epoch 19 test loss 0.4782487452030182\n--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test_loss': 0.47790032625198364}\n--------------------------------------------------------------------------------\n************** iteration 3 **************\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation sanity check: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c20fade359d045ebad1dc666508b401e"}},"metadata":{}},{"name":"stdout","text":"epoch 0 validation loss 0.7757818102836609\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42e5ba0d956446989e60641834394068"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 0 validation loss 1.0586869716644287\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 1 validation loss 0.6514738202095032\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 2 validation loss 0.5546598434448242\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 3 validation loss 0.5191848874092102\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 4 validation loss 0.5203459858894348\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 5 validation loss 0.5543216466903687\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 6 validation loss 0.5371251702308655\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 7 validation loss 0.5020921230316162\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 8 validation loss 0.5310860872268677\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 9 validation loss 0.5163707137107849\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 10 validation loss 0.5199970602989197\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbfc285c27ca4b10b36ed40bd5bb3c00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 12 validation loss 0.49819862842559814\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 13 validation loss 0.5016390681266785\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 14 validation loss 0.506993293762207\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 15 validation loss 0.5037564039230347\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 16 validation loss 0.49378374218940735\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 17 validation loss 0.5002413392066956\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 18 validation loss 0.4941180646419525\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 19 validation loss 0.49516603350639343\npredicting\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db04f59a0c16401a979e655b04acd593"}},"metadata":{}},{"name":"stdout","text":"epoch 19 test loss 0.49378374218940735\n--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test_loss': 0.4916948080062866}\n--------------------------------------------------------------------------------\n************** iteration 4 **************\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation sanity check: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6848188ec321422c88a4cebe207a3936"}},"metadata":{}},{"name":"stdout","text":"epoch 0 validation loss 0.8175176382064819\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76c5f23e8bdc4ae9b2bcfd150d873d6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 0 validation loss 1.098875641822815\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 1 validation loss 0.6376608610153198\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 2 validation loss 0.5344353914260864\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 3 validation loss 0.5257354378700256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 4 validation loss 0.49670976400375366\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 5 validation loss 0.5176390409469604\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 7 validation loss 0.5955437421798706\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 8 validation loss 0.5204251408576965\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 9 validation loss 0.5201459527015686\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 10 validation loss 0.5451594591140747\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 11 validation loss 0.5005325675010681\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 12 validation loss 0.4964125454425812\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 13 validation loss 0.5189399719238281\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 14 validation loss 0.5207920074462891\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 15 validation loss 0.5051115155220032\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 16 validation loss 0.508221447467804\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 17 validation loss 0.5036753416061401\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 18 validation loss 0.5010760426521301\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 19 validation loss 0.4903026819229126\npredicting\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"698f42c9b0654f758ac82d309b49e8c1"}},"metadata":{}},{"name":"stdout","text":"epoch 19 test loss 0.4903026819229126\n--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test_loss': 0.4912501275539398}\n--------------------------------------------------------------------------------\n************** iteration 5 **************\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation sanity check: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00a081e05e0b40159b31a0ca65132952"}},"metadata":{}},{"name":"stdout","text":"epoch 0 validation loss 0.7363137006759644\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11d2ef86be364a3ca87cc0c08e7c078f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 0 validation loss 1.0146530866622925\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 1 validation loss 0.6817278265953064\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 2 validation loss 0.5936357975006104\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 3 validation loss 0.5187280774116516\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 4 validation loss 0.5033672451972961\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 5 validation loss 0.5190524458885193\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 6 validation loss 0.5081059336662292\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 7 validation loss 0.5151649117469788\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 8 validation loss 0.5066143870353699\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 9 validation loss 0.5011321306228638\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 10 validation loss 0.5159334540367126\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 11 validation loss 0.50283282995224\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 12 validation loss 0.513927698135376\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 13 validation loss 0.4902031123638153\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 14 validation loss 0.4838433265686035\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 15 validation loss 0.4907970428466797\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 16 validation loss 0.47520574927330017\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 17 validation loss 0.48754173517227173\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 18 validation loss 0.47380003333091736\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch 19 validation loss 0.46861532330513\npredicting\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2802a341cf5f42fd9c54140e15fa1dae"}},"metadata":{}},{"name":"stdout","text":"epoch 19 test loss 0.46861532330513\n--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test_loss': 0.47284188866615295}\n--------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"(0.47284188866615295 + 0.4912501275539398 + 0.4916948080062866 + 0.47790032625198364 + 0.5041112899780273)/5","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:29:21.666753Z","iopub.execute_input":"2021-07-27T07:29:21.667118Z","iopub.status.idle":"2021-07-27T07:29:21.674279Z","shell.execute_reply.started":"2021-07-27T07:29:21.667083Z","shell.execute_reply":"2021-07-27T07:29:21.673357Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0.4875596880912781"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}