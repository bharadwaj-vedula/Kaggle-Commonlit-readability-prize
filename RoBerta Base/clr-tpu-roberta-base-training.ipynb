{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Versions \n1. Version 1 : basic roberta on TPU, without learning rate scheduler CV- 0.47   LB-\n2. Version 2 : CV:- 0.308  LB:-0.518\n3. Version 3 : CV:-0.3147\n4. Version 4 : Using ragnars method of training, epochs 70 , LR 0.000040\n5. Version 5: using pre trained roberta [MLM] CV: 0.3288 LB:-\n6. Version 6 : changing seed to 123 from 2021 CV:- 0.3128\n7. Version 7: using inspiration from [Notebook](https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch) this has attention block and regression block [help](https://stackoverflow.com/questions/59743161/tensorflow-model-subclassing-mutli-input)  CV:- 0.2953 LB:- 0.532\n8. version 8 :- changing few configs and lr CV:- 0.301 LB\n9. version 9:- using maunish's pretrained MLM model CV:- 0.0.2689 LB:-0.507\n10. Version 10:- used roberta base, with adamw weight decay added shuffle to the dataset CV:-0.217  LB:- 0.484 \n11. Version 11:- same as version 10 , with mauinshs pretrained model CV:- 0.2117","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold,StratifiedKFold,train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.svm import SVR\n\nimport h5py\n\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Layer,Input,LSTM,Bidirectional,Embedding,Dense, Conv1D, Dropout , MaxPool1D , MaxPooling1D, GlobalAveragePooling2D , GlobalAveragePooling1D\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.models import Model,load_model,save_model, model_from_json , Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.activations import tanh, softmax\nimport tensorflow_addons as tfa\n\nfrom transformers import RobertaTokenizerFast,TFRobertaModel,TFAutoModel,AutoConfig\n","metadata":{"id":"K2O9YCthOjjd","execution":{"iopub.status.busy":"2021-07-13T09:07:59.489520Z","iopub.execute_input":"2021-07-13T09:07:59.489960Z","iopub.status.idle":"2021-07-13T09:08:07.845527Z","shell.execute_reply.started":"2021-07-13T09:07:59.489873Z","shell.execute_reply":"2021-07-13T09:08:07.844588Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Hyper parameters","metadata":{"id":"-JWkdZ_hOm7d"}},{"cell_type":"code","source":"max_len = 250\nbatch_size = 32\nAUTOTUNE = tf.data.AUTOTUNE\nSEED = 123\nMODEL=['bert-base-uncased' ,'roberta-base' , 'roberta-large']\n\nmodel_name = MODEL[1]\n\nimport os\nos.makedirs(\"./result\")\n\nsave_dir=\"./result\"","metadata":{"id":"QfFz6Rz1Ol45","execution":{"iopub.status.busy":"2021-07-13T09:08:08.398818Z","iopub.execute_input":"2021-07-13T09:08:08.399155Z","iopub.status.idle":"2021-07-13T09:08:08.404160Z","shell.execute_reply.started":"2021-07-13T09:08:08.399127Z","shell.execute_reply":"2021-07-13T09:08:08.403454Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"paths=[\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\",\n\"/kaggle/input/commonlitreadabilityprize/train.csv\",\n\"/kaggle/input/commonlitreadabilityprize/test.csv\"]\n\ndf_train=pd.read_csv(paths[1])\ndf_test=pd.read_csv(paths[2])\ndf_ss=pd.read_csv(paths[0])","metadata":{"id":"jcM_eaOGOqx5","execution":{"iopub.status.busy":"2021-07-13T09:08:10.738602Z","iopub.execute_input":"2021-07-13T09:08:10.738990Z","iopub.status.idle":"2021-07-13T09:08:10.859246Z","shell.execute_reply.started":"2021-07-13T09:08:10.738958Z","shell.execute_reply":"2021-07-13T09:08:10.858511Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(['url_legal','license','standard_error'],axis='columns')\ndf_test = df_test.drop(['url_legal','license'],axis='columns')","metadata":{"id":"O0mIc8AzOvei","execution":{"iopub.status.busy":"2021-07-13T09:08:12.179207Z","iopub.execute_input":"2021-07-13T09:08:12.179705Z","iopub.status.idle":"2021-07-13T09:08:12.195569Z","shell.execute_reply.started":"2021-07-13T09:08:12.179673Z","shell.execute_reply":"2021-07-13T09:08:12.194486Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_train.columns, df_test.columns","metadata":{"id":"xI3ZW8zfOzWt","outputId":"15148d94-3540-4b2a-e2ae-b897c9064faa","execution":{"iopub.status.busy":"2021-07-13T09:08:12.418741Z","iopub.execute_input":"2021-07-13T09:08:12.419215Z","iopub.status.idle":"2021-07-13T09:08:12.426846Z","shell.execute_reply.started":"2021-07-13T09:08:12.419185Z","shell.execute_reply":"2021-07-13T09:08:12.425944Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(Index(['id', 'excerpt', 'target'], dtype='object'),\n Index(['id', 'excerpt'], dtype='object'))"},"metadata":{}}]},{"cell_type":"code","source":"X= df_train['excerpt']\ny=df_train['target']\n\nX_test = df_test['excerpt']","metadata":{"id":"wkkSaYM7O6wq","execution":{"iopub.status.busy":"2021-07-13T09:08:13.939392Z","iopub.execute_input":"2021-07-13T09:08:13.939890Z","iopub.status.idle":"2021-07-13T09:08:13.945024Z","shell.execute_reply.started":"2021-07-13T09:08:13.939857Z","shell.execute_reply":"2021-07-13T09:08:13.943872Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Define Tokenizer","metadata":{"id":"BPZ30dZ-PLxO"}},{"cell_type":"code","source":"tokenizer = RobertaTokenizerFast.from_pretrained(\"../input/pretrainedrobertabasemlm/mlm_tf-roberta-base\")\ntokenizer.save_pretrained(\"./result/roberta-tokenizer\")","metadata":{"id":"D3CouG00O9Gc","outputId":"e4a5ff4a-c991-4efe-8695-613cf74035fa","execution":{"iopub.status.busy":"2021-07-13T09:08:20.084953Z","iopub.execute_input":"2021-07-13T09:08:20.085521Z","iopub.status.idle":"2021-07-13T09:08:20.472932Z","shell.execute_reply.started":"2021-07-13T09:08:20.085468Z","shell.execute_reply":"2021-07-13T09:08:20.471886Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"('./result/roberta-tokenizer/tokenizer_config.json',\n './result/roberta-tokenizer/special_tokens_map.json',\n './result/roberta-tokenizer/vocab.json',\n './result/roberta-tokenizer/merges.txt',\n './result/roberta-tokenizer/added_tokens.json',\n './result/roberta-tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Dataset Prep","metadata":{"id":"YmIpdQG4PNwM"}},{"cell_type":"code","source":"@tf.function\ndef map_function(encodings , target):\n    input_ids = encodings['input_ids']\n    attention_mask = encodings['attention_mask']\n    \n    target = tf.cast(target, tf.float32 )\n    \n    return {'input_ids': input_ids , 'attention_mask': attention_mask}, target","metadata":{"id":"flHAJrIwPITd","execution":{"iopub.status.busy":"2021-07-13T09:08:23.799136Z","iopub.execute_input":"2021-07-13T09:08:23.799490Z","iopub.status.idle":"2021-07-13T09:08:23.804677Z","shell.execute_reply.started":"2021-07-13T09:08:23.799462Z","shell.execute_reply":"2021-07-13T09:08:23.803981Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{"id":"4m-9cuGw-Dsf"}},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"id":"gWrgGoA5Wouc","outputId":"991714ef-006d-4e39-c4dd-fcbad3946f4c","execution":{"iopub.status.busy":"2021-07-13T09:08:27.540319Z","iopub.execute_input":"2021-07-13T09:08:27.540866Z","iopub.status.idle":"2021-07-13T09:08:33.354410Z","shell.execute_reply.started":"2021-07-13T09:08:27.540816Z","shell.execute_reply":"2021-07-13T09:08:33.353622Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"def create_model(roberta_model):\n  input_layer_id = Input(shape=(max_len,) ,dtype=tf.int32, name = 'input_ids')\n  input_layer_mask = Input(shape=(max_len,) ,dtype=tf.int32, name = 'attention_mask')\n    \n  roberta = roberta_model.roberta(input_ids = input_layer_id , attention_mask = input_layer_mask)[0]\n  roberta_output = roberta[:,0,:]\n    \n  predictions = Dense(1,activation='linear')(roberta_output)\n    \n  model = Model(inputs=[input_layer_id, input_layer_mask] , outputs=predictions)\n  model.compile(\n      optimizer = Adam(learning_rate=  1e-5 ),\n      metrics = RootMeanSquaredError(),\n      loss = \"mse\"\n  )\n  return model\n\"\"\"","metadata":{"id":"KAAWL8Q2PX9X","execution":{"iopub.status.busy":"2021-07-13T07:19:35.775704Z","iopub.execute_input":"2021-07-13T07:19:35.776021Z","iopub.status.idle":"2021-07-13T07:19:35.782593Z","shell.execute_reply.started":"2021-07-13T07:19:35.775992Z","shell.execute_reply":"2021-07-13T07:19:35.781866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaBlock(Layer):\n    def __init__(self, max_len ,name):\n        super(RobertaBlock , self).__init__()\n        self.config = AutoConfig.from_pretrained(name)\n        self.config.update({\"output_hidden_states\":True,\"hidden_dropout_prob\": 0.0, \"layer_norm_eps\": 1e-7})\n        self.roberta_model = TFAutoModel.from_pretrained(name ,from_pt = True, config = self.config)\n        self.dense = Dense(1, activation='linear')\n        \n    def call(self,input_tensors):\n        input_id = input_tensors[0]\n        attention_mask = input_tensors[1]\n        roberta_output = self.roberta_model.roberta(input_ids = input_id , attention_mask = attention_mask)\n        roberta_output = roberta_output.hidden_states[-1]\n        return roberta_output\n    \nclass RegressionHead(Layer):\n    def __init__(self):\n        super(RegressionHead , self).__init__()\n        self.dense = Dense(1, activation=\"linear\")\n    \n    def call(self , input_tensors):\n        x = self.dense(input_tensors)\n        return x\n        \nclass AttentionHead(Layer):\n    def __init__(self):\n        super(AttentionHead , self).__init__()\n        self.dense1 = Dense(512)\n        self.tanh =  tanh\n        self.softmax = softmax\n        self.dense2 = Dense(1,activation=\"softmax\")\n    \n    def call(self , input_tensors):\n        x = self.dense1(input_tensors)\n        x = self.tanh(x)\n        x = self.dense2(x)\n        x = self.softmax(x , axis = 1)\n        return x    \n\n\nclass CLRModel(Model):\n    def __init__(self,max_len,name):\n        super(CLRModel, self).__init__()\n        self.roberta_model = RobertaBlock(max_len , name)\n        self.attentionhead = AttentionHead()\n        self.regressionhead = RegressionHead()\n    \n    def call(self,input_tensors):\n        roberta_output = self.roberta_model(input_tensors)\n        #print('shape of roberta output is' , roberta_output.shape)\n        weights = self.attentionhead(roberta_output)\n        #print('shape of attention head is',weights.shape)\n        context_vector = tf.reduce_sum(weights * roberta_output, axis=1)\n        #print(\"shape of context vector\" , context_vector.shape)\n        x = self.regressionhead(context_vector)\n        return x\n    \n    def model(self):\n        input_id = Input(shape = (max_len, ) ,dtype=tf.int32, name = 'input_ids')\n        attention_mask = Input(shape=(max_len,) ,dtype=tf.int32, name = 'attention_mask')\n        \n        return Model(inputs = [input_id , attention_mask] , outputs = self.call([input_id , attention_mask]))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-13T09:08:33.355540Z","iopub.execute_input":"2021-07-13T09:08:33.355810Z","iopub.status.idle":"2021-07-13T09:08:33.371530Z","shell.execute_reply.started":"2021-07-13T09:08:33.355783Z","shell.execute_reply":"2021-07-13T09:08:33.370847Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model_name","metadata":{"execution":{"iopub.status.busy":"2021-07-13T09:09:21.453393Z","iopub.execute_input":"2021-07-13T09:09:21.453697Z","iopub.status.idle":"2021-07-13T09:09:21.459977Z","shell.execute_reply.started":"2021-07-13T09:09:21.453657Z","shell.execute_reply":"2021-07-13T09:09:21.459309Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'../input/clrp-roberta-base/clrp_roberta_base'"},"metadata":{}}]},{"cell_type":"code","source":"model_name = \"../input/clrp-roberta-base/clrp_roberta_base\"\nwith strategy.scope():\n    model = CLRModel(max_len , model_name).model()\n    \n    \"\"\"input_id = Input(shape = (max_len, ) ,dtype=tf.int32, name = 'input_ids')\n    attention_mask = Input(shape=(max_len,) ,dtype=tf.int32, name = 'attention_mask')\n    output = model([input_id , attention_mask])\n\n    model1 = Model(inputs = [input_id , attention_mask], outputs = output)\"\"\"\n\n    model.compile(\n          optimizer = tfa.optimizers.AdamW(learning_rate=  2e-5 , weight_decay = 1e-7 ),\n          metrics = RootMeanSquaredError(),\n          loss= \"mse\"\n        )\n    \n","metadata":{"id":"-NR56_QOP6OW","outputId":"fc73c80e-d475-49c5-c594-80cd4a812350","execution":{"iopub.status.busy":"2021-07-13T09:08:53.959840Z","iopub.execute_input":"2021-07-13T09:08:53.960182Z","iopub.status.idle":"2021-07-13T09:09:21.452056Z","shell.execute_reply.started":"2021-07-13T09:08:53.960155Z","shell.execute_reply":"2021-07-13T09:09:21.450985Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.bias', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T09:09:33.739507Z","iopub.execute_input":"2021-07-13T09:09:33.740134Z","iopub.status.idle":"2021-07-13T09:09:33.766153Z","shell.execute_reply.started":"2021-07-13T09:09:33.740086Z","shell.execute_reply":"2021-07-13T09:09:33.765307Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_ids (InputLayer)          [(None, 250)]        0                                            \n__________________________________________________________________________________________________\nattention_mask (InputLayer)     [(None, 250)]        0                                            \n__________________________________________________________________________________________________\nroberta_block (RobertaBlock)    (None, 250, 768)     124645632   input_ids[0][0]                  \n                                                                 attention_mask[0][0]             \n__________________________________________________________________________________________________\nattention_head (AttentionHead)  (None, 250, 1)       394241      roberta_block[0][0]              \n__________________________________________________________________________________________________\ntf.math.multiply (TFOpLambda)   (None, 250, 768)     0           attention_head[0][0]             \n                                                                 roberta_block[0][0]              \n__________________________________________________________________________________________________\ntf.math.reduce_sum (TFOpLambda) (None, 768)          0           tf.math.multiply[0][0]           \n__________________________________________________________________________________________________\nregression_head (RegressionHead (None, 1)            769         tf.math.reduce_sum[0][0]         \n==================================================================================================\nTotal params: 125,040,642\nTrainable params: 125,040,642\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"'''json_string = model.to_json()\nwith open(\"model_structure.json\", \"w\") as outfile:\n    outfile.write(json_string)'''","metadata":{"execution":{"iopub.status.busy":"2021-07-13T07:20:10.875835Z","iopub.execute_input":"2021-07-13T07:20:10.876177Z","iopub.status.idle":"2021-07-13T07:20:10.882117Z","shell.execute_reply.started":"2021-07-13T07:20:10.876146Z","shell.execute_reply":"2021-07-13T07:20:10.881174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kfold Training","metadata":{"id":"u0eug6p3Pcr2"}},{"cell_type":"code","source":"scores=[]\niterations = 1\nkfold = KFold(n_splits=5, shuffle= True , random_state = SEED)\nfor train_idx, test_idx in kfold.split(X,y):\n    print(\"************** iteration\",iterations,\"**************\")\n    X_train = X.loc[train_idx]\n    X_test = X.loc[test_idx]\n    y_train = y.loc[train_idx]\n    y_test = y.loc[test_idx]\n    \n    X_train = X_train.tolist()\n    X_test = X_test.tolist()\n\n    y_train = y_train.tolist()\n    y_test = y_test.tolist()\n    \n    #tokenization\n    print('tokenization')\n    train_embeddings = tokenizer(X_train, truncation = True , padding = True , max_length=max_len)\n    test_embeddings = tokenizer(X_test , truncation = True , padding =True , max_length = max_len)\n    \n    #print(train_embeddings.keys())\n    train = tf.data.Dataset.from_tensor_slices((train_embeddings,y_train))\n\n    train = (\n            train\n            .shuffle(buffer_size = 3000)\n            .map(map_function, num_parallel_calls=AUTOTUNE)\n            .batch(24)\n            .prefetch(AUTOTUNE)\n        )\n    \n    test = tf.data.Dataset.from_tensor_slices((test_embeddings , y_test))\n    test = (\n        test\n        .map(map_function, num_parallel_calls = AUTOTUNE)\n        .batch(24)\n        .prefetch(AUTOTUNE)\n    )\n    \n    \n    #Clearing backend session\n    K.clear_session()\n    print(\"Backend Cleared\")\n    \n    early_stopping=EarlyStopping(monitor=\"val_root_mean_squared_error\",min_delta=0,patience=20,verbose=1,mode=\"min\",restore_best_weights=True)\n    #reduce_lr=ReduceLROnPlateau(monitor=\"val_root_mean_squared_error\",factor=0.1,patience=2,min_lr= 1e-8 , verbose=1)\n    model_checkpoint = ModelCheckpoint(f'{save_dir}/roberta_weight_fold_{iterations}.h5',\n                                                        monitor = 'val_root_mean_squared_error', \n                                                 verbose = 1, \n                                                        save_best_only = True,\n                                                        save_weights_only = True, \n                                                        mode = 'min')\n\n    hist=model.fit(train,validation_data=test,epochs= 10,callbacks = [model_checkpoint,early_stopping])\n\n    #prediction\n    print(\"predicting\")\n    model.load_weights(f'{save_dir}/roberta_weight_fold_{iterations}.h5')\n    y_pred = model.predict(test)\n    print(np.sqrt(mse(y_pred,y_test)))\n    scores.append(np.sqrt(mse(y_pred,y_test)))\n    \n    #saving model\n    #print(\"saving model\")\n    #localhost_save_option = tf.saved_model.SaveOptions(experimental_io_device=\"/job:localhost\")\n    #model.save_weights(f'{save_dir}/roberta_weight_fold_{iterations}.h5', options=localhost_save_option)\n    iterations+=1\n    \nprint(\"the final average rmse is \", np.mean(scores))","metadata":{"id":"KW5lqq-bPbrF","outputId":"e0635a28-e23f-4508-974d-581221ecd03d","execution":{"iopub.status.busy":"2021-07-13T09:10:01.542387Z","iopub.execute_input":"2021-07-13T09:10:01.542772Z","iopub.status.idle":"2021-07-13T09:21:59.202011Z","shell.execute_reply.started":"2021-07-13T09:10:01.542738Z","shell.execute_reply":"2021-07-13T09:21:59.200954Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"************** iteration 1 **************\ntokenization\nBackend Cleared\nEpoch 1/10\n95/95 [==============================] - 8s 88ms/step - loss: 0.0064 - root_mean_squared_error: 0.0800 - val_loss: 0.2601 - val_root_mean_squared_error: 0.5100\n\nEpoch 00009: val_root_mean_squared_error did not improve from 0.49725\nEpoch 10/10\n95/95 [==============================] - 8s 88ms/step - loss: 0.0069 - root_mean_squared_error: 0.0832 - val_loss: 0.2498 - val_root_mean_squared_error: 0.4998\n\nEpoch 00010: val_root_mean_squared_error did not improve from 0.49725\npredicting\n0.4972491052767532\n************** iteration 2 **************\ntokenization\nBackend Cleared\nEpoch 1/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.1383 - root_mean_squared_error: 0.3719 - val_loss: 0.1012 - val_root_mean_squared_error: 0.3182\n\nEpoch 00001: val_root_mean_squared_error improved from inf to 0.31820, saving model to ./result/roberta_weight_fold_2.h5\nEpoch 2/10\n95/95 [==============================] - 8s 90ms/step - loss: 0.0655 - root_mean_squared_error: 0.2559 - val_loss: 0.0879 - val_root_mean_squared_error: 0.2965\n\nEpoch 00002: val_root_mean_squared_error improved from 0.31820 to 0.29655, saving model to ./result/roberta_weight_fold_2.h5\nEpoch 3/10\n95/95 [==============================] - 8s 88ms/step - loss: 0.0323 - root_mean_squared_error: 0.1796 - val_loss: 0.0802 - val_root_mean_squared_error: 0.2832\n\nEpoch 00003: val_root_mean_squared_error improved from 0.29655 to 0.28316, saving model to ./result/roberta_weight_fold_2.h5\nEpoch 4/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0162 - root_mean_squared_error: 0.1273 - val_loss: 0.0964 - val_root_mean_squared_error: 0.3104\n\nEpoch 00004: val_root_mean_squared_error did not improve from 0.28316\nEpoch 5/10\n95/95 [==============================] - 9s 97ms/step - loss: 0.0096 - root_mean_squared_error: 0.0982 - val_loss: 0.0865 - val_root_mean_squared_error: 0.2941\n\nEpoch 00005: val_root_mean_squared_error did not improve from 0.28316\nEpoch 6/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0082 - root_mean_squared_error: 0.0905 - val_loss: 0.0813 - val_root_mean_squared_error: 0.2851\n\nEpoch 00006: val_root_mean_squared_error did not improve from 0.28316\nEpoch 7/10\n95/95 [==============================] - 9s 91ms/step - loss: 0.0076 - root_mean_squared_error: 0.0870 - val_loss: 0.0855 - val_root_mean_squared_error: 0.2923\n\nEpoch 00007: val_root_mean_squared_error did not improve from 0.28316\nEpoch 8/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0072 - root_mean_squared_error: 0.0846 - val_loss: 0.0830 - val_root_mean_squared_error: 0.2880\n\nEpoch 00008: val_root_mean_squared_error did not improve from 0.28316\nEpoch 9/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0070 - root_mean_squared_error: 0.0839 - val_loss: 0.0838 - val_root_mean_squared_error: 0.2896\n\nEpoch 00009: val_root_mean_squared_error did not improve from 0.28316\nEpoch 10/10\n95/95 [==============================] - 8s 88ms/step - loss: 0.0063 - root_mean_squared_error: 0.0791 - val_loss: 0.0781 - val_root_mean_squared_error: 0.2795\n\nEpoch 00010: val_root_mean_squared_error improved from 0.28316 to 0.27952, saving model to ./result/roberta_weight_fold_2.h5\npredicting\n0.2795218781034834\n************** iteration 3 **************\ntokenization\nBackend Cleared\nEpoch 1/10\n95/95 [==============================] - 9s 90ms/step - loss: 0.0325 - root_mean_squared_error: 0.1804 - val_loss: 0.0198 - val_root_mean_squared_error: 0.1405\n\nEpoch 00001: val_root_mean_squared_error improved from inf to 0.14054, saving model to ./result/roberta_weight_fold_3.h5\nEpoch 2/10\n95/95 [==============================] - 8s 88ms/step - loss: 0.0174 - root_mean_squared_error: 0.1319 - val_loss: 0.0107 - val_root_mean_squared_error: 0.1035\n\nEpoch 00002: val_root_mean_squared_error improved from 0.14054 to 0.10348, saving model to ./result/roberta_weight_fold_3.h5\nEpoch 3/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0126 - root_mean_squared_error: 0.1122 - val_loss: 0.0166 - val_root_mean_squared_error: 0.1287\n\nEpoch 00003: val_root_mean_squared_error did not improve from 0.10348\nEpoch 4/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0088 - root_mean_squared_error: 0.0939 - val_loss: 0.0078 - val_root_mean_squared_error: 0.0884\n\nEpoch 00004: val_root_mean_squared_error improved from 0.10348 to 0.08842, saving model to ./result/roberta_weight_fold_3.h5\nEpoch 5/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0070 - root_mean_squared_error: 0.0837 - val_loss: 0.0168 - val_root_mean_squared_error: 0.1298\n\nEpoch 00005: val_root_mean_squared_error did not improve from 0.08842\nEpoch 6/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0072 - root_mean_squared_error: 0.0850 - val_loss: 0.0205 - val_root_mean_squared_error: 0.1433\n\nEpoch 00006: val_root_mean_squared_error did not improve from 0.08842\nEpoch 7/10\n95/95 [==============================] - 8s 88ms/step - loss: 0.0081 - root_mean_squared_error: 0.0900 - val_loss: 0.0095 - val_root_mean_squared_error: 0.0975\n\nEpoch 00007: val_root_mean_squared_error did not improve from 0.08842\nEpoch 8/10\n95/95 [==============================] - 9s 99ms/step - loss: 0.0105 - root_mean_squared_error: 0.1025 - val_loss: 0.0153 - val_root_mean_squared_error: 0.1235\n\nEpoch 00008: val_root_mean_squared_error did not improve from 0.08842\nEpoch 9/10\n95/95 [==============================] - 8s 88ms/step - loss: 0.0104 - root_mean_squared_error: 0.1020 - val_loss: 0.0101 - val_root_mean_squared_error: 0.1003\n\nEpoch 00009: val_root_mean_squared_error did not improve from 0.08842\nEpoch 10/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0075 - root_mean_squared_error: 0.0864 - val_loss: 0.0152 - val_root_mean_squared_error: 0.1233\n\nEpoch 00010: val_root_mean_squared_error did not improve from 0.08842\npredicting\n0.08841578239931179\n************** iteration 4 **************\ntokenization\nBackend Cleared\nEpoch 1/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0109 - root_mean_squared_error: 0.1044 - val_loss: 0.0175 - val_root_mean_squared_error: 0.1325\n\nEpoch 00001: val_root_mean_squared_error improved from inf to 0.13247, saving model to ./result/roberta_weight_fold_4.h5\nEpoch 2/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0198 - root_mean_squared_error: 0.1406 - val_loss: 0.0373 - val_root_mean_squared_error: 0.1933\n\nEpoch 00002: val_root_mean_squared_error did not improve from 0.13247\nEpoch 3/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0223 - root_mean_squared_error: 0.1492 - val_loss: 0.0132 - val_root_mean_squared_error: 0.1151\n\nEpoch 00003: val_root_mean_squared_error improved from 0.13247 to 0.11505, saving model to ./result/roberta_weight_fold_4.h5\nEpoch 4/10\n95/95 [==============================] - 8s 88ms/step - loss: 0.0152 - root_mean_squared_error: 0.1235 - val_loss: 0.0090 - val_root_mean_squared_error: 0.0947\n\nEpoch 00004: val_root_mean_squared_error improved from 0.11505 to 0.09473, saving model to ./result/roberta_weight_fold_4.h5\nEpoch 5/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0089 - root_mean_squared_error: 0.0944 - val_loss: 0.0127 - val_root_mean_squared_error: 0.1126\n\nEpoch 00005: val_root_mean_squared_error did not improve from 0.09473\nEpoch 6/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0062 - root_mean_squared_error: 0.0790 - val_loss: 0.0160 - val_root_mean_squared_error: 0.1264\n\nEpoch 00006: val_root_mean_squared_error did not improve from 0.09473\nEpoch 7/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0050 - root_mean_squared_error: 0.0706 - val_loss: 0.0130 - val_root_mean_squared_error: 0.1138\n\nEpoch 00007: val_root_mean_squared_error did not improve from 0.09473\nEpoch 8/10\n95/95 [==============================] - 8s 88ms/step - loss: 0.0056 - root_mean_squared_error: 0.0749 - val_loss: 0.0096 - val_root_mean_squared_error: 0.0978\n\nEpoch 00008: val_root_mean_squared_error did not improve from 0.09473\nEpoch 9/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0054 - root_mean_squared_error: 0.0737 - val_loss: 0.0110 - val_root_mean_squared_error: 0.1049\n\nEpoch 00009: val_root_mean_squared_error did not improve from 0.09473\nEpoch 10/10\n59/95 [=================>............] - ETA: 2s - loss: 0.0058 - root_mean_squared_error: 0.0759","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"0.09473269553744333\n************** iteration 5 **************\ntokenization\nBackend Cleared\nEpoch 1/10\n95/95 [==============================] - 9s 99ms/step - loss: 0.0111 - root_mean_squared_error: 0.1053 - val_loss: 0.0098 - val_root_mean_squared_error: 0.0991\n\nEpoch 00001: val_root_mean_squared_error improved from inf to 0.09910, saving model to ./result/roberta_weight_fold_5.h5\nEpoch 2/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0132 - root_mean_squared_error: 0.1151 - val_loss: 0.0315 - val_root_mean_squared_error: 0.1773\n\nEpoch 00002: val_root_mean_squared_error did not improve from 0.09910\nEpoch 3/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0154 - root_mean_squared_error: 0.1240 - val_loss: 0.0223 - val_root_mean_squared_error: 0.1494\n\nEpoch 00003: val_root_mean_squared_error did not improve from 0.09910\nEpoch 4/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0144 - root_mean_squared_error: 0.1202 - val_loss: 0.0124 - val_root_mean_squared_error: 0.1114\n\nEpoch 00004: val_root_mean_squared_error did not improve from 0.09910\nEpoch 5/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0136 - root_mean_squared_error: 0.1166 - val_loss: 0.0122 - val_root_mean_squared_error: 0.1102\n\nEpoch 00005: val_root_mean_squared_error did not improve from 0.09910\nEpoch 6/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0157 - root_mean_squared_error: 0.1254 - val_loss: 0.0134 - val_root_mean_squared_error: 0.1155\n\nEpoch 00006: val_root_mean_squared_error did not improve from 0.09910\nEpoch 7/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0096 - root_mean_squared_error: 0.0981 - val_loss: 0.0184 - val_root_mean_squared_error: 0.1356\n\nEpoch 00007: val_root_mean_squared_error did not improve from 0.09910\nEpoch 8/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0057 - root_mean_squared_error: 0.0755 - val_loss: 0.0098 - val_root_mean_squared_error: 0.0988\n\nEpoch 00008: val_root_mean_squared_error improved from 0.09910 to 0.09884, saving model to ./result/roberta_weight_fold_5.h5\nEpoch 9/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0043 - root_mean_squared_error: 0.0659 - val_loss: 0.0106 - val_root_mean_squared_error: 0.1028\n\nEpoch 00009: val_root_mean_squared_error did not improve from 0.09884\nEpoch 10/10\n95/95 [==============================] - 8s 89ms/step - loss: 0.0040 - root_mean_squared_error: 0.0631 - val_loss: 0.0122 - val_root_mean_squared_error: 0.1103\n\nEpoch 00010: val_root_mean_squared_error did not improve from 0.09884\npredicting\n0.09883662605035282\nthe final average rmse is  0.2117512174734689\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\njson_string = model.to_json()\nmodel.save_weights('my_model_weights.h5')\nload_weights, load_from_json\n\nwith open(\"sample.json\", \"w\") as outfile:\n    outfile.write(json_object)\n    \nwith open(\"./model_structure.json\", \"r\") as outfile:\n    json=outfile.read()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-07-13T07:27:05.269157Z","iopub.status.idle":"2021-07-13T07:27:05.269769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}